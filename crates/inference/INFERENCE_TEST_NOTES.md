# Inference Test Notes

## ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

### ‚úÖ ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:

1. **‡∏™‡∏£‡πâ‡∏≤‡∏á test program** (`test_inference_pipeline.rs`)
   - ‚úÖ ‡∏£‡∏±‡∏ö RawFramePacket ‡∏à‡∏≤‡∏Å DeckLink
   - ‚úÖ ‡∏™‡πà‡∏á‡πÑ‡∏õ Preprocessing (FP16)
   - ‚úÖ ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ Inference Engine
   - ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• RawDetectionsPacket

2. **Inference Engine Updates**
   - ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° support FP16 input
   - ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á FP16 ‚Üí FP32 ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
   - ‚úÖ ‡πÉ‡∏ä‡πâ default allocators ‡πÅ‡∏ó‡∏ô custom CUDA allocators

3. **Pipeline Integration**
   - ‚úÖ Capture ‚Üí Preprocessing ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
   - ‚úÖ Preprocessing ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (3.59 ms GPU Direct)
   - ‚úÖ Tensor ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö inference

### ‚ö†Ô∏è ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö:

1. **Segmentation Fault**
   - ‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á FP16 ‚Üí FP32
   - ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏: ‡∏≠‡∏≤‡∏à‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° access GPU memory pointer ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
   - FP16 data ‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô GPU memory ‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ dereference ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å CPU

### üîß ‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ:

#### Option 1: ‡πÉ‡∏ä‡πâ CUDA kernel ‡πÅ‡∏õ‡∏•‡∏á FP16 ‚Üí FP32 ‡∏ö‡∏ô GPU
```rust
// ‡πÉ‡∏ä‡πâ CUDA kernel ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏ö‡∏ô CPU
cudarc::driver::launch_kernel(convert_fp16_to_fp32_kernel, ...);
```

#### Option 2: Copy GPU ‚Üí CPU ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏õ‡∏•‡∏á
```rust
// Copy FP16 from GPU to CPU
device.dtoh_sync_copy_into(gpu_fp16_slice, &mut cpu_fp16_buffer)?;

// ‡πÅ‡∏õ‡∏•‡∏á FP16 ‚Üí FP32 ‡∏ö‡∏ô CPU
for (i, &fp16_val) in cpu_fp16_buffer.iter().enumerate() {
    cpu_fp32_buffer[i] = fp16_val.to_f32();
}

// Copy FP32 back to GPU
device.htod_sync_copy_into(&cpu_fp32_buffer, &mut gpu_fp32_buffer)?;
```

#### Option 3: ‡πÉ‡∏´‡πâ ORT ‡∏£‡∏±‡∏ö FP16 ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)
```rust
// ‡∏™‡∏£‡πâ‡∏≤‡∏á FP16 tensor ‡πÅ‡∏ó‡∏ô FP32
let input_tensor = unsafe {
    Tensor::<half::f16>::new(&self.gpu_allocator, shape)?
};

// ‡πÉ‡∏ä‡πâ IO binding ‡∏Å‡∏±‡∏ö FP16 tensor
io_binding.bind_input("images", &input_tensor)?;
```

### üìù ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:

**‡πÉ‡∏ä‡πâ Option 3** ‡πÄ‡∏û‡∏£‡∏≤‡∏∞:
1. Model ‡∏≠‡∏≤‡∏à‡∏£‡∏±‡∏ö FP16 ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (TensorRT FP16 mode)
2. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á ‚Üí ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î latency
3. ‡πÉ‡∏ä‡πâ memory ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ (FP16 = 2 bytes, FP32 = 4 bytes)
4. GPU native FP16 operations ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤

### üéØ Next Steps:

1. ‡πÅ‡∏Å‡πâ inference engine ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö FP16 ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö model input type (‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô FP16)
3. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö inference ‡πÉ‡∏´‡∏°‡πà
4. ‡∏ß‡∏±‡∏î latency ‡∏Ç‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á pipeline

### üìä Performance ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ:

```
Capture:       GPU Direct (zero-copy)
Preprocessing: 3.59 ms ‚ö°
Inference:     (pending fix)
E2E Target:    < 10 ms (100+ FPS)
```

## ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:

```bash
# ‡∏£‡∏±‡∏ô test
cargo run -p playgrounds --bin test_inference_pipeline

# ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ:
# 1. DeckLink device ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
# 2. Model file: crates/inference/YOLOv5.onnx
# 3. GPU ‡∏û‡∏£‡πâ‡∏≠‡∏° TensorRT
```

## ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:

- ‚ö†Ô∏è **‡∏≠‡∏¢‡πà‡∏≤** dereference GPU pointers ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å CPU code
- ‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á FP16 ‚Üî FP32 ‡∏ö‡∏ô GPU data ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ CUDA kernel ‡∏´‡∏£‡∏∑‡∏≠ copy ‡∏°‡∏≤ CPU ‡∏Å‡πà‡∏≠‡∏ô
- ‚úÖ ‡πÉ‡∏ä‡πâ FP16 native ‡∏ï‡∏•‡∏≠‡∏î pipeline ‡∏à‡∏∞‡πÉ‡∏´‡πâ performance ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

---

**‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞**: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç (FP16 conversion issue)  
**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà**: October 14, 2025
